{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d70d33c7-e993-42fd-9755-b478fe56fe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.quantization\n",
    "\n",
    "# Set device (use CUDA if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# MNIST dataset with normalization\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59e9d61-85e1-40f5-b38b-eef2597cfff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "\n",
    "# Define the transformation to normalize MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load MNIST training dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# Load MNIST testing dataset\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Combine datasets\n",
    "combined_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "import random\n",
    "\n",
    "def split_dataset(combined_dataset, N):\n",
    "    # Total number of samples in the dataset\n",
    "    M = len(combined_dataset)\n",
    "    indices = list(range(M))\n",
    "    split_size = M // N\n",
    "    \n",
    "    # Shuffle indices to ensure randomness in splitting\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Split indices into N parts\n",
    "    user_data = [indices[i * split_size:(i + 1) * split_size] for i in range(N)]\n",
    "    \n",
    "    # Create subsets for each user\n",
    "    user_datasets = [torch.utils.data.Subset(combined_dataset, user_data[i]) for i in range(N)]\n",
    "    \n",
    "    return user_datasets\n",
    "\n",
    "\n",
    "def split_train_test(user_dataset, test_ratio=0.2):\n",
    "    # Total number of samples\n",
    "    M = len(user_dataset)\n",
    "    test_size = int(M * test_ratio)\n",
    "    train_size = M - test_size\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_subset, test_subset = random_split(user_dataset, [train_size, test_size])\n",
    "    \n",
    "    return train_subset, test_subset\n",
    "\n",
    "\n",
    "user_datasets = split_dataset(combined_dataset, N)\n",
    "\n",
    "batch_size = 64  # Adjust batch size as needed\n",
    "\n",
    "# Split user-specific dataset into training and testing sets\n",
    "user_train_loaders = []\n",
    "user_test_loaders = []\n",
    "for user_dataset in user_datasets:\n",
    "    train_data, test_data = split_train_test(user_dataset)\n",
    "    user_train_loaders.append(DataLoader(train_data, batch_size=batch_size, shuffle=True))\n",
    "    user_test_loaders.append(DataLoader(test_data, batch_size=batch_size, shuffle=False))\n",
    "    \n",
    "def aggregate_updates(local_updates):\n",
    "    new_state_dict = {}\n",
    "    for key in local_updates[0].keys():\n",
    "        # Determine the original dtype\n",
    "        original_dtype = local_updates[0][key].dtype\n",
    "\n",
    "        # Compute the mean in float32, then convert back to the original dtype\n",
    "        new_state_dict[key] = torch.mean(\n",
    "            torch.stack([update[key].float() for update in local_updates]), dim=0\n",
    "        ).to(original_dtype)\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2247ae28-7a7e-49f3-9cb4-71759ad5f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        # DeQuantStub converts tensors from quantized to floating point\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.pool(self.relu1(self.conv1(x)))\n",
    "        x = self.pool(self.relu2(self.conv2(x)))\n",
    "        x = x.reshape(-1, 64 * 8 * 8)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8642f7-7f5f-40cd-9e37-9783deeab44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = Net().to(device)\n",
    "\n",
    "client_models = [Net().to(device) for i in range(N)]\n",
    "\n",
    "for i, model in enumerate(client_models + [global_model]):\n",
    "    # Fuse layers (for better optimization during quantization)\n",
    "    model.fuse_model = lambda: torch.quantization.fuse_modules(model, [[\"conv1\", \"relu1\"], [\"conv2\", \"relu2\"]])\n",
    "    model.fuse_model()\n",
    "    \n",
    "    # Specify quantization configuration\n",
    "    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "    \n",
    "    # Prepare for QAT\n",
    "    model = torch.quantization.prepare_qat(model)\n",
    "    #model = torch.quantization.convert(model)\n",
    "    # Update the corresponding model\n",
    "    if i < len(client_models):\n",
    "        client_models[i] = model\n",
    "    else:\n",
    "        global_model = model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e063b09-66c3-4c2f-b2b5-3f2378b293a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1 - Epoch 1/100 - Train Loss: 2.3013, Train Accuracy: 11.12% - Test Loss: 2.2937, Test Accuracy: 15.00%\n",
      "User 2 - Epoch 1/100 - Train Loss: 2.3012, Train Accuracy: 11.83% - Test Loss: 2.2921, Test Accuracy: 14.00%\n",
      "User 3 - Epoch 1/100 - Train Loss: 2.3000, Train Accuracy: 10.29% - Test Loss: 2.2927, Test Accuracy: 13.00%\n",
      "User 4 - Epoch 1/100 - Train Loss: 2.2987, Train Accuracy: 11.29% - Test Loss: 2.2968, Test Accuracy: 16.33%\n",
      "User 5 - Epoch 1/100 - Train Loss: 2.3000, Train Accuracy: 10.58% - Test Loss: 2.2941, Test Accuracy: 13.83%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 91\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed - \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Avg Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_test_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Avg Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_test_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# global_model = Net().to(device)\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m \u001b[43mfederated_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_train_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_test_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m, in \u001b[0;36mfederated_train\u001b[1;34m(global_model, client_models, user_train_loaders, user_test_loaders, epochs)\u001b[0m\n\u001b[0;32m     27\u001b[0m output \u001b[38;5;241m=\u001b[39m client_models[i](data)\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(output, target)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Accumulate training loss and accuracy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_train_losses_q = []  # Average training loss per epoch\n",
    "avg_train_accs_q = []    # Average training accuracy per epoch\n",
    "avg_test_losses_q = []   # Average testing loss per epoch\n",
    "avg_test_accs_q = []     # Average testing accuracy per epoch\n",
    "\n",
    "def federated_train(global_model, client_models, user_train_loaders, user_test_loaders, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        local_updates = []\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        test_losses = []\n",
    "        test_accs = []\n",
    "\n",
    "        for i in range(N):\n",
    "            # Copy the global model for each user\n",
    "            client_models[i].load_state_dict(global_model.state_dict())\n",
    "            optimizer = torch.optim.SGD(client_models[i].parameters(), lr=0.01)  # Example with SGD optimizer\n",
    "\n",
    "            user_train_loss = 0.0\n",
    "            user_train_correct = 0\n",
    "            user_train_samples = 0\n",
    "\n",
    "            # Train the local model on the user's training data\n",
    "            client_models[i].train()\n",
    "            for data, target in user_train_loaders[i]:\n",
    "                optimizer.zero_grad()\n",
    "                output = client_models[i](data)\n",
    "                loss = torch.nn.functional.cross_entropy(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate training loss and accuracy\n",
    "                user_train_loss += loss.item() * len(data)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                user_train_correct += (predicted == target).sum().item()\n",
    "                user_train_samples += len(data)\n",
    "\n",
    "            # Calculate and store the training loss and accuracy for this user\n",
    "            train_losses.append(user_train_loss / user_train_samples)\n",
    "            train_accs.append(100 * user_train_correct / user_train_samples)\n",
    "\n",
    "            # Save the local model state dict\n",
    "            local_updates.append(client_models[i].state_dict())\n",
    "\n",
    "            # Evaluate on the test set of the user\n",
    "            user_test_loss = 0.0\n",
    "            user_test_correct = 0\n",
    "            user_test_samples = 0\n",
    "\n",
    "            client_models[i].eval()\n",
    "            with torch.no_grad():\n",
    "                for data, target in user_test_loaders[i]:\n",
    "                    output = client_models[i](data)\n",
    "                    loss = torch.nn.functional.cross_entropy(output, target)\n",
    "                    user_test_loss += loss.item() * len(data)\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    user_test_correct += (predicted == target).sum().item()\n",
    "                    user_test_samples += len(data)\n",
    "\n",
    "            # Calculate and store the testing loss and accuracy for this user\n",
    "            test_losses.append(user_test_loss / user_test_samples)\n",
    "            test_accs.append(100 * user_test_correct / user_test_samples)\n",
    "\n",
    "            print(f\"User {i+1} - Epoch {epoch+1}/{epochs} - \"\n",
    "                  f\"Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accs[-1]:.2f}% - \"\n",
    "                  f\"Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {test_accs[-1]:.2f}%\")\n",
    "\n",
    "        # Aggregate updates from all users\n",
    "        aggregated_model_state = aggregate_updates(local_updates)\n",
    "        global_model.load_state_dict(aggregated_model_state)\n",
    "\n",
    "        # Calculate epoch-level averages and store them\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        avg_train_acc = sum(train_accs) / len(train_accs)\n",
    "        avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "        avg_test_acc = sum(test_accs) / len(test_accs)\n",
    "\n",
    "        avg_train_losses_q.append(avg_train_loss)\n",
    "        avg_train_accs_q.append(avg_train_acc)\n",
    "        avg_test_losses_q.append(avg_test_loss)\n",
    "        avg_test_accs_q.append(avg_test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed - \"\n",
    "              f\"Avg Train Loss: {avg_train_loss:.4f}, Avg Train Accuracy: {avg_train_acc:.2f}%, \"\n",
    "              f\"Avg Test Loss: {avg_test_loss:.4f}, Avg Test Accuracy: {avg_test_acc:.2f}%\")\n",
    "    \n",
    "\n",
    "\n",
    "# Example usage\n",
    "# global_model = Net().to(device)\n",
    "federated_train(global_model, client_models, user_train_loaders, user_test_loaders, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adde05-747a-4dbc-91a6-f15280b50cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quantized = torch.quantization.convert(global_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1b000-26fa-4519-ad79-7d9cae8e202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate quantized model\n",
    "model_quantized.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_quantized(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the quantized model on the test set: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021efb3-bed7-4f30-9a7e-154ff9b0ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the metrics as subplots\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Subplot 1: Accuracy\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(avg_train_accs_q, label='Avg Train Accuracy', marker='s')\n",
    "plt.plot(avg_test_accs_q, label='Avg Test Accuracy', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Federated Training Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Subplot 2: Loss\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(avg_train_losses_q, label='Avg Train Loss', marker='o')\n",
    "plt.plot(avg_test_losses_q, label='Avg Test Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Federated Training Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f98a1-5b9d-4d53-9bab-4bc73ee3eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu1(self.conv1(x)))\n",
    "        x = self.pool(self.relu2(self.conv2(x)))\n",
    "        x = x.reshape(-1, 64 * 8 * 8)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "global_model = Net().to(device)\n",
    "\n",
    "client_models = [Net().to(device) for i in range(N)]\n",
    "\n",
    "\n",
    "\n",
    "avg_train_losses = []  # Average training loss per epoch\n",
    "avg_train_accs = []    # Average training accuracy per epoch\n",
    "avg_test_losses = []   # Average testing loss per epoch\n",
    "avg_test_accs = []     # Average testing accuracy per epoch\n",
    "\n",
    "def federated_train(global_model, client_models, user_train_loaders, user_test_loaders, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        local_updates = []\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        test_losses = []\n",
    "        test_accs = []\n",
    "\n",
    "        for i in range(N):\n",
    "            # Copy the global model for each user\n",
    "            client_models[i].load_state_dict(global_model.state_dict())\n",
    "            optimizer = torch.optim.SGD(client_models[i].parameters(), lr=0.01)  # Example with SGD optimizer\n",
    "\n",
    "            user_train_loss = 0.0\n",
    "            user_train_correct = 0\n",
    "            user_train_samples = 0\n",
    "\n",
    "            # Train the local model on the user's training data\n",
    "            client_models[i].train()\n",
    "            for data, target in zip(user_train_loaders[i]):\n",
    "                optimizer.zero_grad()\n",
    "                output = client_models[i](data)\n",
    "                loss = torch.nn.functional.cross_entropy(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate training loss and accuracy\n",
    "                user_train_loss += loss.item() * len(data)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                user_train_correct += (predicted == target).sum().item()\n",
    "                user_train_samples += len(data)\n",
    "\n",
    "            # Calculate and store the training loss and accuracy for this user\n",
    "            train_losses.append(user_train_loss / user_train_samples)\n",
    "            train_accs.append(100 * user_train_correct / user_train_samples)\n",
    "\n",
    "            # Save the local model state dict\n",
    "            local_updates.append(client_models[i].state_dict())\n",
    "\n",
    "            # Evaluate on the test set of the user\n",
    "            user_test_loss = 0.0\n",
    "            user_test_correct = 0\n",
    "            user_test_samples = 0\n",
    "\n",
    "            client_models[i].eval()\n",
    "            with torch.no_grad():\n",
    "                for data, target in user_test_loaders[i]:\n",
    "                    output = client_models[i](data)\n",
    "                    loss = torch.nn.functional.cross_entropy(output, target)\n",
    "                    user_test_loss += loss.item() * len(data)\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    user_test_correct += (predicted == target).sum().item()\n",
    "                    user_test_samples += len(data)\n",
    "\n",
    "            # Calculate and store the testing loss and accuracy for this user\n",
    "            test_losses.append(user_test_loss / user_test_samples)\n",
    "            test_accs.append(100 * user_test_correct / user_test_samples)\n",
    "\n",
    "            print(f\"User {i+1} - Epoch {epoch+1}/{epochs} - \"\n",
    "                  f\"Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accs[-1]:.2f}% - \"\n",
    "                  f\"Test Loss: {test_losses[-1]:.4f}, Test Accuracy: {test_accs[-1]:.2f}%\")\n",
    "\n",
    "        # Aggregate updates from all users\n",
    "        aggregated_model_state = aggregate_updates(local_updates)\n",
    "        global_model.load_state_dict(aggregated_model_state)\n",
    "\n",
    "        # Calculate epoch-level averages and store them\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        avg_train_acc = sum(train_accs) / len(train_accs)\n",
    "        avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "        avg_test_acc = sum(test_accs) / len(test_accs)\n",
    "\n",
    "        avg_train_losses.append(avg_train_loss)\n",
    "        avg_train_accs.append(avg_train_acc)\n",
    "        avg_test_losses.append(avg_test_loss)\n",
    "        avg_test_accs.append(avg_test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed - \"\n",
    "              f\"Avg Train Loss: {avg_train_loss:.4f}, Avg Train Accuracy: {avg_train_acc:.2f}%, \"\n",
    "              f\"Avg Test Loss: {avg_test_loss:.4f}, Avg Test Accuracy: {avg_test_acc:.2f}%\")\n",
    "    \n",
    "    # Plotting the metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(range(1, epochs + 1), avg_train_losses, label='Avg Train Loss', marker='o')\n",
    "    plt.plot(range(1, epochs + 1), avg_test_losses, label='Avg Test Loss', marker='o')\n",
    "    plt.plot(range(1, epochs + 1), avg_train_accs, label='Avg Train Accuracy', marker='s')\n",
    "    plt.plot(range(1, epochs + 1), avg_test_accs, label='Avg Test Accuracy', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.title('Federated Training Metrics')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# global_model = Net().to(device)\n",
    "federated_train(global_model, client_models, user_train_loaders, user_test_loaders, epochs=100)\n",
    "\n",
    "\n",
    "# Evaluate quantized model\n",
    "model_quantized.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_quantized(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the quantized model on the test set: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd435cf-f22c-4824-8c2a-87da18c79ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the metrics as subplots\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Subplot 1: Accuracy\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(avg_train_accs_q, label='Avg Train Accuracy Quantized', marker='s')\n",
    "plt.plot(avg_test_accs_q, label='Avg Test Accuracy Quantized', marker='s')\n",
    "plt.plot(avg_test_accs, label='Avg Test Accuracy', marker='s')\n",
    "plt.plot(avg_train_accs, label='Avg Train Accuracy', marker='s')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Federated Training Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Subplot 2: Loss\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(avg_train_losses_q, label='Avg Train Loss Quantized', marker='o')\n",
    "plt.plot(avg_test_losses_q, label='Avg Test Loss Quantized', marker='o')\n",
    "plt.plot(avg_train_losses, label='Avg Train Loss', marker='o')\n",
    "plt.plot(avg_test_losses, label='Avg Test Loss', marker='o')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Federated Training Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35ff4e-7b57-4d24-86f1-c721ae9e51d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elems = sum([p.numel() for p in model_quantized.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04cf58-c409-49ef-81f2-12bfe3992850",
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.numel() for p in model_prepared.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576cf5a-2588-4d3c-bf1c-916fb8ba912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58e76f-ae8b-4b02-957a-c4655d07dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "num_users = 20                 # Number of users\n",
    "area_size = 1000                # 100 x 100 m^2 area\n",
    "Pt = 100e-3                    # Transmission power (100 mW)\n",
    "B = 2e6                        # Bandwidth (2 MHz)\n",
    "N0 = 1e-9                      # Noise spectral density (10^-9 W/Hz)\n",
    "model_size_bits = 32 * 1e6     # Model size: 32-bit representation of 1M elements\n",
    "\n",
    "# Fixed server position (e.g., at the center of the area)\n",
    "server_x, server_y = area_size / 2, area_size / 2\n",
    "\n",
    "# Monte Carlo simulation to calculate average bit rate\n",
    "num_iterations = 1000          # Number of Monte Carlo iterations\n",
    "bit_rates = []                 # To store bit rates for each iteration\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Randomly distribute users in 100x100 area\n",
    "    user_x_coords = np.random.uniform(0, area_size, num_users)\n",
    "    user_y_coords = np.random.uniform(0, area_size, num_users)\n",
    "    \n",
    "    # Calculate distances to the server\n",
    "    distances = np.sqrt((user_x_coords - server_x)**2 + (user_y_coords - server_y)**2)\n",
    "    distances = np.clip(distances, 1, None)  # Avoid zero distance; minimum distance = 1 m\n",
    "    \n",
    "    # Calculate achievable bit rates for all users\n",
    "    rates = B * np.log2(1 + (Pt / (distances**2 * B * N0)))\n",
    "    \n",
    "    # Compute average bit rate across all users in this iteration\n",
    "    avg_rate = np.mean(rates)\n",
    "    bit_rates.append(avg_rate)\n",
    "\n",
    "# Calculate the final average bit rate over all iterations\n",
    "avg_bit_rate = np.mean(bit_rates)\n",
    "print(f\"Average Bit Rate: {avg_bit_rate:.2f} bps\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79ad69-2eb3-4004-82a4-52c4c3cd3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate energy required to transmit the model to the server\n",
    "transmission_time = model_size_bits / avg_bit_rate  # Time to transmit the model\n",
    "energy_per_round = Pt * transmission_time  # Energy consumption\n",
    "\n",
    "print(f\"Energy Required to Transmit the Model: {energy_per_round:.2f} Joules per round\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d5145-b04e-4fa1-9d96-32239280758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_acc = 65.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04812fd2-a567-43da-b64e-540cc32e4e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_acc_rounds_qunatized = sum([i < target_acc for i in avg_test_accs_q])\n",
    "target_acc_rounds = sum([i < target_acc for i in avg_test_accs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711f712-06c7-41f5-b45e-edf1ef4915df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy for the quantized model to achieve 65 Accuracy Accuracy\n",
    "total_energy_quantized = target_acc_rounds_qunatized * energy_per_round / 4\n",
    "\n",
    "# Energy for normal model to achieve 65 Accuracy\n",
    "total_energy = target_acc_rounds * energy_per_round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebf173-8d64-418e-a0b1-0f169a897132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total energy for quantized model to achieve {target_acc} %: {total_energy_quantized:.2f} Joules\")\n",
    "print(f\"Total energy for full precision model to achieve {target_acc} %: {total_energy:.2f} Joules\")\n",
    "\n",
    "energy_savings = (total_energy - total_energy_quantized) / total_energy\n",
    "print(f\"Energy savings: {energy_savings * 100:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d00da-cb00-408c-a364-b793034ab978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da2f31-d768-41b0-902c-ed05d572d5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
