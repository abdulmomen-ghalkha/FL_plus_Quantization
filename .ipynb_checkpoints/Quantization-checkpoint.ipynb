{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "765d1518-f65c-4608-8fce-b1ef8a57b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.quantization\n",
    "import os\n",
    "\n",
    "# Set device (use CUDA if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# MNIST dataset with normalization\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff88ca73-34a3-453d-a680-2137ea27e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "# Define the transformation to normalize MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load MNIST training dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Load MNIST testing dataset\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Combine datasets\n",
    "combined_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "import random\n",
    "\n",
    "def split_dataset(combined_dataset, N):\n",
    "    # Total number of samples in the dataset\n",
    "    M = len(combined_dataset)\n",
    "    indices = list(range(M))\n",
    "    split_size = M // N\n",
    "    \n",
    "    # Shuffle indices to ensure randomness in splitting\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Split indices into N parts\n",
    "    user_data = [indices[i * split_size:(i + 1) * split_size] for i in range(N)]\n",
    "    \n",
    "    # Create subsets for each user\n",
    "    user_datasets = [torch.utils.data.Subset(combined_dataset, user_data[i]) for i in range(N)]\n",
    "    \n",
    "    return user_datasets\n",
    "\n",
    "\n",
    "def split_train_test(user_dataset, test_ratio=0.2):\n",
    "    # Total number of samples\n",
    "    M = len(user_dataset)\n",
    "    test_size = int(M * test_ratio)\n",
    "    train_size = M - test_size\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_subset, test_subset = random_split(user_dataset, [train_size, test_size])\n",
    "    \n",
    "    return train_subset, test_subset\n",
    "\n",
    "\n",
    "user_datasets = split_dataset(combined_dataset, N)\n",
    "\n",
    "batch_size = 64  # Adjust batch size as needed\n",
    "\n",
    "# Split user-specific dataset into training and testing sets\n",
    "user_train_loaders = []\n",
    "user_test_loaders = []\n",
    "for user_dataset in user_datasets:\n",
    "    train_data, test_data = split_train_test(user_dataset)\n",
    "    user_train_loaders.append(DataLoader(train_data, batch_size=batch_size, shuffle=True))\n",
    "    user_test_loaders.append(DataLoader(test_data, batch_size=batch_size, shuffle=False))\n",
    "    \n",
    "def aggregate_updates(local_updates):\n",
    "    # A naive method to aggregate model weights\n",
    "    new_state_dict = {}\n",
    "    for key in local_updates[0].keys():\n",
    "        new_state_dict[key] = torch.mean(torch.stack([update[key] for update in local_updates]), dim=0)\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36d02e5a-2a28-4c2d-98ce-94319972202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.pool(self.relu1(self.conv1(x)))\n",
    "        x = self.pool(self.relu2(self.conv2(x)))\n",
    "        x = x.reshape(-1, 64 * 7 * 7)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "047b6017-2ce6-4ca3-8dd0-91cd2d69ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = Net().to(device)\n",
    "\n",
    "client_models = [Net().to(device) for i in range(N)]\n",
    "\n",
    "for i, model in enumerate(client_models + [global_model]):\n",
    "    # Fuse layers (for better optimization during quantization)\n",
    "    model.fuse_model = lambda: torch.quantization.fuse_modules(model, [[\"conv1\", \"relu1\"], [\"conv2\", \"relu2\"]])\n",
    "    model.fuse_model()\n",
    "    \n",
    "    # Specify quantization configuration\n",
    "    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "    \n",
    "    # Prepare for QAT\n",
    "    model = torch.quantization.prepare_qat(model)\n",
    "    model = torch.quantization.convert(model)\n",
    "    # Update the corresponding model\n",
    "    if i < len(client_models):\n",
    "        client_models[i] = model\n",
    "    else:\n",
    "        global_model = model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c1058cdf-5656-429f-8648-f83f47d18eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d6e05af-4ba4-4407-b214-577e54b2208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43 MB\n"
     ]
    }
   ],
   "source": [
    "print_model_size(global_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05bfc7a6-34af-4b60-a48d-8249c25455a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant.scale: tensor([1.])\n",
      "quant.zero_point: tensor([0])\n",
      "conv1.weight: tensor([[[[-0.1022, -0.0723,  0.2443],\n",
      "          [ 0.0324, -0.3191,  0.2369],\n",
      "          [-0.3166,  0.2568,  0.1870]]],\n",
      "\n",
      "\n",
      "        [[[-0.1043,  0.2284,  0.0496],\n",
      "          [-0.3078, -0.1390, -0.3178],\n",
      "          [ 0.3078, -0.1167, -0.0025]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3239,  0.0612,  0.2117],\n",
      "          [-0.0944, -0.2678,  0.0332],\n",
      "          [ 0.0332, -0.2933, -0.2219]]],\n",
      "\n",
      "\n",
      "        [[[-0.3064,  0.1293, -0.2059],\n",
      "          [-0.2011, -0.1771,  0.1077],\n",
      "          [-0.2059,  0.1532, -0.2202]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0491,  0.0801, -0.1550],\n",
      "          [ 0.1860,  0.3151,  0.3280],\n",
      "          [-0.0594,  0.0207,  0.1705]]],\n",
      "\n",
      "\n",
      "        [[[-0.1298,  0.2621,  0.1578],\n",
      "          [-0.1628,  0.1247, -0.3231],\n",
      "          [-0.2239, -0.2799,  0.2722]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2907, -0.3153, -0.0788],\n",
      "          [-0.2932,  0.2882,  0.0591],\n",
      "          [ 0.1380,  0.2611,  0.3104]]],\n",
      "\n",
      "\n",
      "        [[[-0.2488, -0.1009, -0.1367],\n",
      "          [-0.0650,  0.1972,  0.2241],\n",
      "          [ 0.2847,  0.0045,  0.1547]]],\n",
      "\n",
      "\n",
      "        [[[-0.0681,  0.2545, -0.1763],\n",
      "          [ 0.1603,  0.0561,  0.2144],\n",
      "          [-0.1102,  0.0341, -0.0100]]],\n",
      "\n",
      "\n",
      "        [[[-0.2739,  0.2241,  0.2422],\n",
      "          [-0.1290, -0.1562,  0.2060],\n",
      "          [ 0.0951,  0.2332, -0.2898]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2301, -0.1763,  0.2494],\n",
      "          [ 0.2537, -0.0323, -0.1699],\n",
      "          [-0.2752,  0.2688,  0.2666]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0192,  0.0724, -0.2237],\n",
      "          [-0.2726,  0.1725, -0.2322],\n",
      "          [-0.0895, -0.1065,  0.2194]]],\n",
      "\n",
      "\n",
      "        [[[-0.0344, -0.2680, -0.1489],\n",
      "          [ 0.0000,  0.2909, -0.1970],\n",
      "          [-0.2772, -0.1443, -0.0046]]],\n",
      "\n",
      "\n",
      "        [[[-0.0689, -0.2068, -0.2849],\n",
      "          [-0.1838,  0.2688, -0.2941],\n",
      "          [ 0.0574,  0.1953,  0.0436]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1813,  0.2896, -0.1319],\n",
      "          [ 0.1013,  0.2849,  0.2637],\n",
      "          [-0.1342, -0.0377,  0.2991]]],\n",
      "\n",
      "\n",
      "        [[[-0.2146,  0.1323, -0.0200],\n",
      "          [ 0.1772,  0.0225,  0.0175],\n",
      "          [-0.1547, -0.3169, -0.0599]]],\n",
      "\n",
      "\n",
      "        [[[-0.2814, -0.2451, -0.0931],\n",
      "          [-0.2179,  0.2383, -0.2882],\n",
      "          [ 0.2633,  0.2020, -0.1634]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1699,  0.0786, -0.1979],\n",
      "          [-0.3247, -0.2055,  0.2866],\n",
      "          [-0.1141,  0.0127, -0.3171]]],\n",
      "\n",
      "\n",
      "        [[[-0.1328, -0.0169,  0.1014],\n",
      "          [ 0.3041, -0.3090, -0.2100],\n",
      "          [-0.0362,  0.0193, -0.1086]]],\n",
      "\n",
      "\n",
      "        [[[-0.2456,  0.2734,  0.0185],\n",
      "          [ 0.1529, -0.0116, -0.1506],\n",
      "          [ 0.0487, -0.2966, -0.0579]]],\n",
      "\n",
      "\n",
      "        [[[-0.2903,  0.0467, -0.3149],\n",
      "          [ 0.1894, -0.0935, -0.0320],\n",
      "          [-0.2878,  0.2755,  0.0369]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1676, -0.2242, -0.2650],\n",
      "          [ 0.2876,  0.1608,  0.0544],\n",
      "          [ 0.0340, -0.0634, -0.2853]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0554, -0.1283,  0.3196],\n",
      "          [-0.2315, -0.0453,  0.0705],\n",
      "          [ 0.3196, -0.0277,  0.1283]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2570, -0.2550, -0.0749],\n",
      "          [-0.1943,  0.1498,  0.2550],\n",
      "          [-0.0283,  0.0040,  0.0789]]],\n",
      "\n",
      "\n",
      "        [[[-0.1202, -0.1629,  0.0175],\n",
      "          [-0.2463,  0.2424, -0.0834],\n",
      "          [ 0.2346,  0.0970, -0.1125]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0077, -0.1469, -0.3298],\n",
      "          [-0.0747, -0.0644,  0.3272],\n",
      "          [ 0.2036,  0.2886,  0.2061]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3298,  0.0130,  0.0493],\n",
      "          [ 0.1844,  0.2130, -0.0260],\n",
      "          [ 0.2857, -0.3039,  0.1377]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0410,  0.0000, -0.1255],\n",
      "          [ 0.1689, -0.1785, -0.1520],\n",
      "          [-0.0121,  0.2437,  0.3064]]],\n",
      "\n",
      "\n",
      "        [[[-0.2778, -0.1433, -0.1064],\n",
      "          [ 0.0499, -0.0239,  0.1867],\n",
      "          [-0.0434, -0.2106,  0.1064]]],\n",
      "\n",
      "\n",
      "        [[[-0.1603, -0.0207, -0.3310],\n",
      "          [ 0.0000, -0.1189, -0.1939],\n",
      "          [ 0.1319, -0.2715,  0.2844]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1835,  0.3149, -0.2901],\n",
      "          [-0.1859,  0.0248,  0.0099],\n",
      "          [-0.1091, -0.1686,  0.2306]]],\n",
      "\n",
      "\n",
      "        [[[-0.0845,  0.3253, -0.2152],\n",
      "          [-0.1691, -0.2357,  0.1306],\n",
      "          [-0.0871, -0.1409, -0.0205]]]], size=(32, 1, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0025, 0.0025, 0.0026, 0.0024, 0.0026, 0.0025, 0.0025, 0.0022, 0.0020,\n",
      "        0.0023, 0.0022, 0.0021, 0.0023, 0.0023, 0.0024, 0.0025, 0.0023, 0.0025,\n",
      "        0.0024, 0.0023, 0.0025, 0.0023, 0.0025, 0.0020, 0.0019, 0.0026, 0.0026,\n",
      "        0.0024, 0.0022, 0.0026, 0.0025, 0.0026], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0)\n",
      "conv1.bias: Parameter containing:\n",
      "tensor([ 0.1567,  0.2724,  0.0905, -0.2091, -0.1733,  0.1711,  0.2463, -0.3290,\n",
      "         0.1819, -0.1314, -0.0718,  0.1923,  0.3048, -0.2345, -0.0517,  0.1485,\n",
      "        -0.0102, -0.1964, -0.1587, -0.1160,  0.1789,  0.2803,  0.1476,  0.1901,\n",
      "        -0.2632, -0.3325, -0.1430,  0.1925,  0.1944, -0.2061,  0.1732, -0.0728],\n",
      "       requires_grad=True)\n",
      "conv1.scale: 1.0\n",
      "conv1.zero_point: 0\n",
      "conv2.weight: tensor([[[[ 0.0360, -0.0508, -0.0245],\n",
      "          [-0.0055, -0.0042, -0.0088],\n",
      "          [-0.0379, -0.0273,  0.0065]],\n",
      "\n",
      "         [[ 0.0037, -0.0014,  0.0573],\n",
      "          [ 0.0079, -0.0171,  0.0296],\n",
      "          [ 0.0573,  0.0379,  0.0494]],\n",
      "\n",
      "         [[ 0.0420,  0.0563,  0.0540],\n",
      "          [-0.0023, -0.0185, -0.0448],\n",
      "          [-0.0009,  0.0129,  0.0411]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0268,  0.0194, -0.0531],\n",
      "          [-0.0434, -0.0139,  0.0434],\n",
      "          [ 0.0189,  0.0217, -0.0217]],\n",
      "\n",
      "         [[ 0.0254,  0.0333,  0.0060],\n",
      "          [-0.0286,  0.0060, -0.0134],\n",
      "          [ 0.0139,  0.0379, -0.0046]],\n",
      "\n",
      "         [[ 0.0111, -0.0466, -0.0079],\n",
      "          [-0.0185,  0.0092,  0.0365],\n",
      "          [ 0.0296, -0.0069,  0.0079]]],\n",
      "\n",
      "\n",
      "        [[[-0.0551, -0.0082, -0.0100],\n",
      "          [-0.0082,  0.0369, -0.0246],\n",
      "          [ 0.0314,  0.0187, -0.0556]],\n",
      "\n",
      "         [[-0.0506, -0.0141,  0.0296],\n",
      "          [ 0.0059,  0.0551,  0.0492],\n",
      "          [-0.0232, -0.0205,  0.0501]],\n",
      "\n",
      "         [[ 0.0487, -0.0173, -0.0578],\n",
      "          [-0.0036,  0.0451,  0.0323],\n",
      "          [-0.0287,  0.0196, -0.0073]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0182,  0.0342, -0.0424],\n",
      "          [-0.0405, -0.0323, -0.0159],\n",
      "          [ 0.0114, -0.0492,  0.0332]],\n",
      "\n",
      "         [[-0.0428,  0.0355, -0.0164],\n",
      "          [-0.0214, -0.0483,  0.0205],\n",
      "          [-0.0210, -0.0469, -0.0519]],\n",
      "\n",
      "         [[-0.0547,  0.0528, -0.0455],\n",
      "          [-0.0301,  0.0351,  0.0387],\n",
      "          [-0.0255, -0.0419,  0.0137]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0259, -0.0005, -0.0203],\n",
      "          [-0.0397, -0.0540, -0.0005],\n",
      "          [-0.0494,  0.0457,  0.0365]],\n",
      "\n",
      "         [[-0.0115,  0.0009, -0.0503],\n",
      "          [ 0.0305, -0.0240, -0.0494],\n",
      "          [ 0.0356, -0.0157, -0.0014]],\n",
      "\n",
      "         [[-0.0074, -0.0236,  0.0171],\n",
      "          [ 0.0411, -0.0319,  0.0346],\n",
      "          [-0.0531,  0.0582, -0.0577]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0577,  0.0282,  0.0296],\n",
      "          [-0.0079,  0.0328, -0.0333],\n",
      "          [ 0.0573,  0.0277,  0.0365]],\n",
      "\n",
      "         [[-0.0369, -0.0231,  0.0277],\n",
      "          [ 0.0568, -0.0162, -0.0291],\n",
      "          [ 0.0060,  0.0005,  0.0508]],\n",
      "\n",
      "         [[ 0.0148, -0.0009, -0.0351],\n",
      "          [ 0.0032, -0.0277, -0.0522],\n",
      "          [ 0.0042,  0.0143,  0.0402]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0110,  0.0467,  0.0179],\n",
      "          [ 0.0417, -0.0023, -0.0087],\n",
      "          [ 0.0403,  0.0453,  0.0316]],\n",
      "\n",
      "         [[ 0.0215, -0.0453,  0.0527],\n",
      "          [-0.0476, -0.0096, -0.0050],\n",
      "          [ 0.0092, -0.0069, -0.0275]],\n",
      "\n",
      "         [[ 0.0389, -0.0481, -0.0234],\n",
      "          [ 0.0517, -0.0321, -0.0069],\n",
      "          [ 0.0275,  0.0147,  0.0298]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0536, -0.0101,  0.0124],\n",
      "          [-0.0224, -0.0353,  0.0472],\n",
      "          [ 0.0060, -0.0513,  0.0568]],\n",
      "\n",
      "         [[-0.0009, -0.0293, -0.0114],\n",
      "          [ 0.0463,  0.0156, -0.0380],\n",
      "          [ 0.0238,  0.0069, -0.0568]],\n",
      "\n",
      "         [[-0.0348, -0.0412, -0.0343],\n",
      "          [-0.0060, -0.0334, -0.0179],\n",
      "          [-0.0197, -0.0330,  0.0426]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0476,  0.0503, -0.0360],\n",
      "          [ 0.0577,  0.0176,  0.0305],\n",
      "          [-0.0120, -0.0139, -0.0092]],\n",
      "\n",
      "         [[ 0.0060, -0.0374,  0.0005],\n",
      "          [ 0.0245,  0.0374, -0.0563],\n",
      "          [-0.0462, -0.0333,  0.0083]],\n",
      "\n",
      "         [[-0.0189, -0.0088, -0.0517],\n",
      "          [ 0.0060,  0.0550,  0.0102],\n",
      "          [-0.0471, -0.0425,  0.0139]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0333, -0.0065,  0.0042],\n",
      "          [ 0.0587, -0.0527,  0.0079],\n",
      "          [-0.0231,  0.0203, -0.0568]],\n",
      "\n",
      "         [[-0.0494,  0.0240, -0.0014],\n",
      "          [-0.0129,  0.0291, -0.0513],\n",
      "          [ 0.0356, -0.0272, -0.0346]],\n",
      "\n",
      "         [[-0.0406,  0.0236, -0.0231],\n",
      "          [ 0.0559,  0.0508,  0.0573],\n",
      "          [ 0.0139,  0.0152,  0.0032]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0483, -0.0267,  0.0497],\n",
      "          [ 0.0547, -0.0041,  0.0175],\n",
      "          [ 0.0506, -0.0087,  0.0538]],\n",
      "\n",
      "         [[ 0.0092, -0.0037,  0.0161],\n",
      "          [-0.0262,  0.0147, -0.0570],\n",
      "          [-0.0005, -0.0014, -0.0271]],\n",
      "\n",
      "         [[-0.0037,  0.0156, -0.0396],\n",
      "          [ 0.0331, -0.0055,  0.0483],\n",
      "          [-0.0373, -0.0262, -0.0129]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0069, -0.0432, -0.0069],\n",
      "          [ 0.0281, -0.0198,  0.0152],\n",
      "          [ 0.0580, -0.0083,  0.0501]],\n",
      "\n",
      "         [[ 0.0230,  0.0327,  0.0322],\n",
      "          [-0.0212, -0.0074, -0.0235],\n",
      "          [ 0.0492,  0.0106, -0.0221]],\n",
      "\n",
      "         [[-0.0138,  0.0074,  0.0308],\n",
      "          [ 0.0120, -0.0354, -0.0156],\n",
      "          [ 0.0294, -0.0345, -0.0538]]]], size=(64, 32, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
      "        0.0005], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0)\n",
      "conv2.bias: Parameter containing:\n",
      "tensor([-0.0149,  0.0360,  0.0052,  0.0320, -0.0396, -0.0301, -0.0524, -0.0095,\n",
      "         0.0189,  0.0099,  0.0047, -0.0114,  0.0131,  0.0556, -0.0328, -0.0100,\n",
      "         0.0021, -0.0120, -0.0537, -0.0262,  0.0482, -0.0331,  0.0155,  0.0134,\n",
      "         0.0057, -0.0137,  0.0448, -0.0495,  0.0287,  0.0445, -0.0377, -0.0061,\n",
      "         0.0308, -0.0396,  0.0425, -0.0147, -0.0154, -0.0508,  0.0407, -0.0492,\n",
      "        -0.0546, -0.0583, -0.0430, -0.0431,  0.0554,  0.0448, -0.0122, -0.0457,\n",
      "        -0.0243,  0.0511, -0.0518, -0.0522,  0.0451,  0.0275,  0.0436, -0.0394,\n",
      "         0.0459,  0.0415,  0.0076,  0.0388, -0.0371, -0.0574, -0.0248,  0.0381],\n",
      "       requires_grad=True)\n",
      "conv2.scale: 1.0\n",
      "conv2.zero_point: 0\n",
      "fc1.scale: 1.0\n",
      "fc1.zero_point: 0\n",
      "fc1._packed_params.dtype: torch.qint8\n",
      "fc1._packed_params._packed_params: (tensor([[-0.0111,  0.0164, -0.0038,  ...,  0.0119, -0.0039,  0.0098],\n",
      "        [ 0.0160,  0.0003, -0.0069,  ..., -0.0123,  0.0140,  0.0134],\n",
      "        [-0.0088,  0.0097,  0.0147,  ..., -0.0164, -0.0130,  0.0144],\n",
      "        ...,\n",
      "        [-0.0057, -0.0092, -0.0113,  ...,  0.0049,  0.0134,  0.0137],\n",
      "        [ 0.0155,  0.0069, -0.0041,  ...,  0.0143, -0.0099, -0.0125],\n",
      "        [-0.0151,  0.0092, -0.0077,  ...,  0.0109,  0.0155,  0.0134]],\n",
      "       size=(128, 3136), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "        0.0001, 0.0001], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0), Parameter containing:\n",
      "tensor([-0.0171,  0.0132, -0.0031,  0.0137,  0.0059,  0.0102, -0.0141,  0.0169,\n",
      "         0.0055, -0.0152,  0.0171,  0.0153,  0.0041,  0.0131,  0.0009, -0.0162,\n",
      "        -0.0074,  0.0130,  0.0029,  0.0006, -0.0175, -0.0048, -0.0122,  0.0124,\n",
      "        -0.0167,  0.0005, -0.0007,  0.0132, -0.0156, -0.0099, -0.0117, -0.0109,\n",
      "        -0.0064, -0.0072, -0.0124,  0.0070,  0.0130, -0.0028,  0.0112, -0.0073,\n",
      "         0.0152, -0.0010, -0.0074, -0.0151,  0.0070, -0.0085, -0.0077,  0.0178,\n",
      "         0.0142,  0.0097,  0.0100, -0.0049,  0.0016,  0.0073, -0.0138, -0.0076,\n",
      "         0.0092,  0.0171,  0.0111,  0.0167,  0.0164,  0.0071, -0.0097,  0.0058,\n",
      "         0.0161,  0.0027, -0.0118, -0.0061, -0.0123, -0.0104,  0.0160, -0.0078,\n",
      "        -0.0013, -0.0163, -0.0101,  0.0004, -0.0092, -0.0121, -0.0117,  0.0169,\n",
      "        -0.0071, -0.0164, -0.0074, -0.0167, -0.0065,  0.0146,  0.0023, -0.0092,\n",
      "        -0.0130,  0.0067, -0.0059, -0.0024,  0.0151,  0.0130, -0.0106,  0.0043,\n",
      "        -0.0015, -0.0022, -0.0055,  0.0129, -0.0054,  0.0007,  0.0160,  0.0043,\n",
      "        -0.0033, -0.0023,  0.0128,  0.0170,  0.0129,  0.0057, -0.0149,  0.0113,\n",
      "         0.0112,  0.0027,  0.0142,  0.0063, -0.0044,  0.0167,  0.0160,  0.0117,\n",
      "        -0.0080, -0.0103,  0.0093,  0.0137,  0.0087,  0.0082, -0.0012, -0.0087],\n",
      "       requires_grad=True))\n",
      "fc2.scale: 1.0\n",
      "fc2.zero_point: 0\n",
      "fc2._packed_params.dtype: torch.qint8\n",
      "fc2._packed_params._packed_params: (tensor([[ 0.0356, -0.0294, -0.0116,  ...,  0.0547, -0.0534, -0.0773],\n",
      "        [-0.0374,  0.0194, -0.0721,  ..., -0.0582,  0.0520,  0.0790],\n",
      "        [ 0.0359,  0.0490, -0.0242,  ..., -0.0048, -0.0297,  0.0622],\n",
      "        ...,\n",
      "        [-0.0768, -0.0816,  0.0449,  ..., -0.0470, -0.0415,  0.0304],\n",
      "        [ 0.0431, -0.0493, -0.0534,  ..., -0.0438,  0.0075,  0.0561],\n",
      "        [ 0.0450,  0.0389,  0.0273,  ..., -0.0832,  0.0164, -0.0095]],\n",
      "       size=(10, 128), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
      "        0.0007], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), axis=0), Parameter containing:\n",
      "tensor([-0.0609,  0.0251, -0.0327,  0.0621,  0.0173,  0.0351, -0.0225, -0.0358,\n",
      "         0.0560, -0.0745], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# Extract quantized parameters\n",
    "for name, param in global_model.state_dict().items():\n",
    "    print(f\"{name}: {param}\")\n",
    "    # Quantized weights have scale and zero-point for reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9438e236-9e4f-4608-99c1-157da45433e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdu_\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "C:\\Users\\abdu_\\anaconda3\\envs\\fmtl_sheaves\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:1272: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize and quantize the model\n",
    "model = Net()\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # Quantization config\n",
    "model = torch.quantization.prepare(model)  # Prepare for quantization\n",
    "quantized_model = torch.quantization.convert(model)  # Convert to 8-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34efd518-caf5-45c8-b491-594e0de1526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_precision_model = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "076203cf-6455-4785-8afb-5a9809213894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43 MB\n",
      "1.69 MB\n"
     ]
    }
   ],
   "source": [
    "print_model_size(quantized_model)\n",
    "print_model_size(full_precision_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1606ccf5-2658-487c-9ca2-d47141549641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a8fe112-219e-4ab4-a2ef-b6aef0091d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new full-precision model\n",
    "\n",
    "# Dequantize each layer manually\n",
    "for name, param in quantized_model.named_parameters():\n",
    "    if hasattr(param, 'q_per_channel_scales'):\n",
    "        # Handle per-channel quantization\n",
    "        dequantized_weight = param.dequantize()\n",
    "    elif hasattr(param, 'q_scale'):\n",
    "        # Handle per-tensor quantization\n",
    "        scale, zero_point = param.q_scale(), param.q_zero_point()\n",
    "        dequantized_weight = (param.int_repr().float() - zero_point) * scale\n",
    "    else:\n",
    "        dequantized_weight = param.float()\n",
    "\n",
    "    # Assign back to the full-precision model\n",
    "    target_layer = dict(full_precision_model.named_parameters())[name]\n",
    "    target_layer.data.copy_(dequantized_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3692a87-f8e9-40af-9c47-8e2fe6e6465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Output: torch.Size([64, 10])\n",
      "Dequantized Output: torch.Size([64, 10])\n",
      "Quantized Output: torch.Size([64, 10])\n",
      "Dequantized Output: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "for j, (x, y) in zip(range(2), user_train_loaders[0]):\n",
    "    # Check the outputs\n",
    "    quantized_output = model(x)\n",
    "    dequantized_output = full_precision_model(x)\n",
    "    \n",
    "    print(\"Quantized Output:\", quantized_output.shape)\n",
    "    print(\"Dequantized Output:\", dequantized_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c8d53-8ee9-47b7-a178-2d280f2a8ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
